\documentclass[12pt,a4paper,twocolumn]{article}

\usepackage[top=0.8in, left=0.6in, right=0.6in, bottom=0.8in]{geometry}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{mathtools}
\usepackage[]{algorithm2e}

\author{Dionisio Perez-Mavrogenis}
\title{Review of \emph{Mining the Network Behaviour of Bots}}
	
\begin{document}
	\maketitle
	\section*{\emph{Abstract}}
	The purpose of this report is to give a overview of the Hierarchical Clustering (HC) learning algorithm as applied in \citep{cavallaro} and give an overview of its algorithmic properties, benefits and shortcomings.\citeauthor{cavallaro} present a framework for automatically extracting network activity profiles for bot clients(in terms of the network traffic and communication patterns they use) in order to incorporate these behavioural models as rules for NIDSs\footnote{NIDS = Network Intrusion Detection System} for detecting infected computers in a network.
	
	\section{Introduction}
\citet{cavallaro}(the authors) are proposing a novel method for robustly mining the interesting part of a bot's($B$) network behaviour and formulating a model by exploiting the observation that typically bots exhibit periodic behaviour. To this end the timing dependencies between network events, actions dependent on network events and recurring events are mined from a network trace, $T_B$, generated by $B$ in a controlled environment. The authors cluster both the network flows for each bot, as well as the timing differences of the flows in each cluster, in order to identify the bot's core behaviour.

	 The clustering of the trace is broken down into three parts: the \emph{intra-cluster} analysis yields $C_{periodic}$(a set of periodic network flows), \emph{inter-cluster dependency} analysis yields $C_{reconsidered}$(an attempt to reconsidered ignored clusters as they were found to depend on elements of $C_{periodic}$) and finally \emph{inter-trace} analysis which outputs cluster$C_{interesting}$(clusters that are \emph{network-similar} according to a similarity measure devised by the authors between different runs of the bot sample). 
	
	The authors have developed and tested a proof of concept implementation which provides satisfactory results and indicates that the current analysis method could be incorporate into the real-world defences deployed to guard networks. Accurately identifying bot-infected hosts is of high interest and importance both to businesses as well as individuals, as botnets are used to carry out extortion, DDOS, send SPAM, perform bank fraud and much more. 
	
	\section{Problem Statement}
	The problem that the authors are trying to address is trying to accurately identify even individually infected hosts on a network that are part of a botnet with a low false positive rate in a way which is independent of the botnet structure or its communication mechanisms(i.e. encrypted or plain-text).
	
	Furthermore once the network behaviour of a given bot sample is extracted, the authors proceed to express this behaviour as a set of network rules that can be used by the popular open source NIDS \emph{Bro}\footnote{Bro website : \href{https://www.bro.org/}{https://www.bro.org/}}. The quality of these rules is directly dependent on the quality of the clustering.
	
	The authors attempt to address the above issues by following an approach that is somewhat different from the existing \emph{vertical} and \emph{horizontal correlation} detection methods. In vertical correlation the network events and traffic are being inspected for known malicious patterns, whereas in horizontal correlation the traffic between two clients is correlated in order to identify similar patterns or anomalous behaviour. In contrast, the authors avoid DPI\footnote{DPI = Deep Packet Inspection} and instead focus mostly on coarse-grained flow features as shown in Table \ref{table:features_used}(discussed further in Section \ref{sec:discussion}).
	
\begin{table}
	\begin{tabular}{ c | c | c}
	\textbf{Network Feature} & \textbf{Granularity} & \textbf{Used}\\
	\hline
	\hline
	Byte Sent & c & yes \\
	Byte Recv & c & yes \\
	\# Pkts per Flow & f & yes \\
	Destination IP & c & yes \\
	Destination Port & c & yes \\
	Pkt inter-arrival time & f & no \\
	Pkt key-words & f & no \\
	
	\end{tabular}
	\caption{Flow clustering features used. Under \emph{Granularity} \texttt{c} denotes coarse and \texttt{f} denotes fine-grained.}
	\label{table:features_used}
\end{table}

	\section{Algorithm Explanation}
	The authors are using a hierarchical clustering algorithm(unsupervised learning) with average linkage and their distance function $d(.,.)$ considers the Euclidean distance over the set of normalised network flows, $F$, shown in Equation \ref{equation:distance}:
	
\begin{equation}
d(a,b) = \frac{1}{|F|} \sum_{x,y \in F} \sqrt{f(x,y)^2} \quad \forall a,b \in flows\\
\label{equation:distance}
\end{equation}
\begin{equation*}
f(x,y) = \left\{ 
        \begin{array}{l l}
            |x-y| & \quad \text{if x,y} \in [0,1]\\
            0 & \quad \text{if x,y} \in IP \wedge x = y\\
            1 & \quad \text{otherwise}\\
        \end{array} \right.
\end{equation*}
	
	The authors do not explicitly state the type of HC algorithm they are using, however the text suggests that they are using an agglomerative HC. An overview of an agglomerative HC is given in Algorithm \ref{algo:HC}, and explained in greater depth in \citep{survey}. Given an input with $N$ elements, the output of a HC algorithm is a dendrogram, with each data point being a leaf in the tree and the whole data-set being represented as the root and useful information can be viewed by cutting the dendrogram at different levels( review this? ).
	
\begin{algorithm}
\KwData{Network flows}
\KwResult{Dendrogram of clusters of the network flows.}
Initialise N Singleton clusters\\
Build distance matrix for all clusters.\\
\While{number of clusters $>$ 1}{
	Merge 2 closest clusters\\
	Update distance matrix.
}
Output resulting cluster.\\

\caption{Outline of agglomerative hierarchical clustering for N input patterns.}
\label{algo:HC}
\end{algorithm}	
	
	
	The space complexity of the algorithm is $O(N^2)$, where $N$ is the number of input patterns (network flows in our example). The dimensionality, $M$, of the input is considered when calculating the distance between two data points and so the time complexity(number of calculations) of the algorithm  could be approximated by $O(M\times N^2)$ number of calculations in order to calculate the distances between the data points.	
	
	\section{Discussion}
	\label{sec:discussion}
	In this scenario an unsupervised learning algorithm the intuitive choice, as the traffic's origins are known to be malicious (and hence need no classification). Furthermore the choice of using hierarchical clustering rather than hard partitioning makes sense, as traffic could be aggregated at different levels and a flow could belong in more than one clusters. The most common criticism on classical HC algorithms is their inability to scale and cope with big data-sets, their sensitivity to outliers and their inability to represent faithfully clusters of irregular shapes(as they tend to form sphere-shaped clusters).
	
	In order to deal with the issue of outliers and data scaling alternatives such as BIRCH\citep{BIRCH} have been developed, which has complexity $O(N)$. Algorithms like CURE\citep{CURE} were constructed in order to explore more complicated cluster shapes than spherical structures. 	\citeauthor{cavallaro} chose to use average linkage in order to avoid the chain effect observed in \emph{single linkage} or the sensitivity to outliers exhibited by \emph{complete linkage} and perform an initial filtering step that removes known noisy traffic (such as SPAM or scan attempts) in order to keep the processed data to a minimum, as well as using a small number of features (feature selection and input representation is critical in malware classification [\citep{Hou201055}, \citep{IP_classificaton}, \citep{syscalls}]).
	
	One might wonder why a more efficient algorithm was not used. Implementing a classic HC is easy, although inefficient, and the visualisation of data at various dendrogram cuts might reveal different information and is easily interpreted. Alternatives such a BIRCH tend to throw away outliers, possibly discarding a critical piece of information. As mentioned by the authors bots tend to perform evasive actions, i.e. actions that would not normally be performed in order to throw off NDISs or learning algorithms and thwart analysis. Such actions might be well masked and even a human expert might not be able to readily understand why the bot does what it does. It is critical therefore to not throw away information that is not readily classified as periodic or interesting (and hence the intra-trace cluster correlation step).

The authors have tested their prototype and had an approximate detection rate of 50\% which is a positive result, suggesting that there is room for improvement and the possibility to incorporate this framework in a production environment.
	\bibliographystyle{plain}
	\bibliography{aml_cw1_report}
\end{document}
